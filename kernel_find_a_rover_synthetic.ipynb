{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3OlX7kn1Mue"
      },
      "source": [
        "# Lab: UAV-assisted wireless localization\n",
        "\n",
        "_Fraida Fund_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRVbUAj64KsW"
      },
      "source": [
        "* **Net ID**:\n",
        "* **Name**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bOheeQI1UCH"
      },
      "source": [
        "You've been asked to contribute your machine learning expertise to a crucial and potentially life-saving mission.\n",
        "\n",
        "A pair of hikers has gone missing in a national park, and they are believed to be critically injured. Fortunately, they have activated a wireless locator beacon, and it transmits a constant wireless signal from their location. Unfortunately, their beacon was not able to get a satellite fix, so their GPS position is not known.\n",
        "\n",
        "To rescue the injured hikers, therefore, their location must be estimated using the signal strength of the wireless signal from the beacon: when a radio receiver is close to the beacon, the signal strength will be high. When a radio receiver is far from the beacon, the signal strength will be lower.  (The relationship is noisy, however; the wireless signal also fluctuates over time, even with a constant distance.)\n",
        "\n",
        "You are going to fly an unmanned aerial vehicle (UAV) with a radio receiver around the area where they were last seen, and use the received wireless signal strength to fit a machine learning model that will estimate the hikers' position. Then, you'll relay this information to rescuers, who will try to reach that position by land. (Unfortunately, due to dense tree cover, the UAV will not be able to visually confirm their position.)\n",
        "\n",
        "There is a complication, though - the UAV has a limited battery life, and therefore, limited flight time. You'll have to get an accurate estimate of the hikers' position in a very short time!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781atfrtfus0"
      },
      "source": [
        "---\n",
        "\n",
        "#### Objectives\n",
        "\n",
        "In this experiment, you will:\n",
        "\n",
        "* observe how the RBF kernel is used in a Gaussian Process Regression\n",
        "* observe how the Gaussian Process Regression approximates the true function\n",
        "* observe how Bayesian Optimization is used to decide which data point to acquire next.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LigNAxqqeOX"
      },
      "source": [
        "---\n",
        "\n",
        "#### Citations\n",
        "\n",
        "This experiment uses the Bayesian Optimization implementation of\n",
        "\n",
        ">  Fernando Nogueira, \"Bayesian Optimization: Open source constrained global optimization tool for Python,\" 2014. Available: https://github.com/fmfn/BayesianOptimization\n",
        "\n",
        "\n",
        "The figures in the background section are from:\n",
        "\n",
        "> Wang, Jie. \"An intuitive tutorial to Gaussian processes regression.\" Computing in Science & Engineering (2023). https://arxiv.org/abs/2009.10862\n",
        "\n",
        "with contributions by Yufei Zhen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4KJQuHLq58s"
      },
      "outputs": [],
      "source": [
        "!pip install bayesian-optimization==2.0.0 numpy==1.26.4 scikit_learn==1.5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running the cell above, use Runtime > Restart Session from the Colab menu. Then, continue with the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw1hR9ly8hSv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from bayes_opt import BayesianOptimization, acquisition\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, RBF\n",
        "\n",
        "import geopy.distance\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "from ipywidgets import interact, fixed, widgets\n",
        "from mpl_toolkits import mplot3d\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "plt.rcParams['axes.formatter.useoffset'] = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mV80lct1W5g"
      },
      "source": [
        "## Framing the problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3bC1Axr1b5e"
      },
      "source": [
        "We are going to estimate the hikers' position based on the premise that the received signal strength is highest when the UAV is at the same latitude and longitude as the hikers.\n",
        "\n",
        "We will frame our machine learning problem as follows:\n",
        "\n",
        "* features $X$: latitude, longitude\n",
        "* target variable $y$: received signal strength\n",
        "\n",
        "In other words, given a coordinate (latitude and longitude) we want to predict the received signal strength at that location.\n",
        "\n",
        "However, we don't really care if our model is bad at predicting the signal strength in places where it is low! Our *true* goal is to predict where the target variable will be highest. We will decide how \"good\" our model is by computing the mean squared error of the position estimate: the distance between the true location of the hikers, and the coordinate that our model predicts has the highest received signal strength."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b136u0eNj_Cc"
      },
      "source": [
        "#### Gaussian Process\n",
        "\n",
        "\n",
        "A Gaussian process is a stochastic process (a collection of random variables), where\n",
        "\n",
        "* any point $\\mathbf{x} \\in \\mathbb{R}^d$ is assigned a random variable $f(\\mathbf{x})$\n",
        "* the joint distribution of a finite number of these variables $p(f(\\mathbf{x}_1),...,f(\\mathbf{x}_N))$ is itself Gaussian, i.e. has a multivariate normal distribution:\n",
        "\n",
        "$$ p(\\mathbf{f} \\lvert \\mathbf{X}) = \\mathcal{N}(\\mathbf{f} \\lvert \\boldsymbol\\mu, \\mathbf{K})$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcCfIjsC5kCv"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "In the expression above,\n",
        "\n",
        "* $\\mathbf{f} = (f(\\mathbf{x}_1),...,f(\\mathbf{x}_N))$ are the random variables,\n",
        "* $\\boldsymbol\\mu = (m(\\mathbf{x}_1),...,m(\\mathbf{x}_N))$ where $m$ is a mean function,\n",
        "* and $K_{ij} = k(\\mathbf{x}_i,\\mathbf{x}_j)$ $k$ is a *kernel function* or *covariance function*.\n",
        "\n",
        "\n",
        "Informally, you may think of a Gaussian process as a distribution over functions, whose shape (for example, the smoothness) is defined by the kernel function.\n",
        "\n",
        "If two points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are considered similar by the kernel, then we expect the function at these points, $f(\\mathbf{x}_i)$ and $f(\\mathbf{x}_j)$, to also have similar values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJMuwnJGkA9D"
      },
      "source": [
        "#### Gaussian Process Regression\n",
        "\n",
        "Given that a Gaussian process is a distribution over functions, we can think about the mean of the Gaussian process as the mean over all of these functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy5oCJrE9BMp"
      },
      "source": [
        "For example, suppose we will use the kernel function\n",
        "\n",
        "$$k(x_i,x_j)  = \\exp\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)$$\n",
        "\n",
        "where $d(.)$ is the Euclidean distance and $l$ is a length scale hyperparameter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTr1pvVe9Dcy"
      },
      "outputs": [],
      "source": [
        "def rbf_kernel(a, b, l=0.1):\n",
        "    sqdist = np.sum(a**2,axis=1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T)\n",
        "    # note: uses the fact that ||a-b||^2 = ||a||^2 + ||b||^2 - 2 * a^T * b\n",
        "    return np.exp((-1/(2*l)) * sqdist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOa60pba-ed-"
      },
      "source": [
        "If we would sample *one* function from the Gaussian process defined by this kernel (and with zero mean), it might look like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdyF2XEJ_JDH"
      },
      "outputs": [],
      "source": [
        "@interact(m = widgets.IntSlider(min=1, max=100, step=1, value=1, description=\"Functions:\"),\n",
        "          l = widgets.FloatLogSlider(value=0.1,  base=10,  min=-2, max=2, step=0.1, description='Kernel l:'),\n",
        "          show_mean = widgets.Checkbox(value=False,  description='Show mean and variance' ),\n",
        "          show_samp = widgets.Checkbox(value=True,  description='Show sampled functions' )\n",
        "          )\n",
        "def sample_prior(m = 1, l=0.1, show_mean = False, show_samp = True):\n",
        "  n = 50 #   # n points in the range of (0, 1)\n",
        "\n",
        "  # compute means and kernel\n",
        "  mean = np.zeros(n)\n",
        "  Xshow = np.linspace(0, 1, n).reshape(-1,1)\n",
        "  K_ = rbf_kernel(Xshow, Xshow, l=l)\n",
        "  # now sample from multivariate normal m times\n",
        "  f_prior = np.random.multivariate_normal(mean, K_, 1000).T\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "\n",
        "  # show the mean and standard deviation across all the sampled functions\n",
        "  if show_mean:\n",
        "    mean_samples = f_prior.mean(axis=1)\n",
        "    var_samples  = f_prior.var(axis=1)\n",
        "    plt.plot(Xshow, mean_samples, c='black')\n",
        "    plt.gca().fill_between(Xshow.flat, mean_samples+var_samples, mean_samples-var_samples,\n",
        "                        color ='grey', alpha=0.2 )\n",
        "\n",
        "  # show the first m samples from the prior distribution\n",
        "  if show_samp:\n",
        "    for i in range(m):\n",
        "        plt.plot(Xshow, f_prior[:,i], '-o', c='tab:blue',  alpha=0.3 )\n",
        "\n",
        "  plt.ylim(-3,3)\n",
        "  plt.xlim(0,1)\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_zzUS_bIoOI"
      },
      "source": [
        "You can change the value of the kernel hyperparameter $l$ using the slider, to see the effect. For the RBF kernel, the length scale hyperparameter determines how fast the function varies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNikrCaUM9w_"
      },
      "source": [
        "Now suppose we were to sample *many* functions from the Gaussian Process. Try it - move the \"Functions\" slider all the way to the right, to show a large number of Gaussian random variables sampled from the Gaussian process.\n",
        "\n",
        "The mean of the Gaussian process is the expectation across all of the (infinite) functions. Check the \"Show mean and variance\" box to see it the sample mean and variance across 1000 sampled functions from the Gaussian process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKH-a64N4Oms"
      },
      "source": [
        "This is the idea of the *prior* in our Gaussian process regression. Initially, our model considers each of these functions to be equally probable, and its prediction is the expectation across all of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgyIe_YsWC6M"
      },
      "source": [
        "However, as we sample points from the \"true function\", we no longer consider every function in the Gaussian process to be equally probable.\n",
        "\n",
        "For example, if we observe a data point that tells us that the value of the \"true function\" at $x = 0.5$,\n",
        "\n",
        "$$t(0.5) = 1$$\n",
        "\n",
        "we would note that only some of the functions in the Gaussian process are consistent with that observation. This is the idea of the *posterior* -  our current belief based on the existing observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXqe3m7yZ4vq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "n = 50 #   # n points in the range of (0, 1)\n",
        "m = 20\n",
        "\n",
        "# compute means and kernel\n",
        "mean = np.zeros(n)\n",
        "Xshow = np.linspace(0, 1, n).reshape(-1,1)\n",
        "K_ = rbf_kernel(Xshow, Xshow, l=0.05)\n",
        "\n",
        "# samples from prior\n",
        "f_prior = np.random.multivariate_normal(mean, K_, 10000).T\n",
        "mean_prior = f_prior.mean(axis=1)\n",
        "var_prior  = f_prior.var(axis=1)\n",
        "\n",
        "# now from posterior, given that t(0.5) = true_val\n",
        "true_val = 1\n",
        "samples_mid = f_prior[int(n/2), :]\n",
        "post = np.argwhere((samples_mid >=  true_val-1e-2) & (samples_mid <= true_val+1e-2))\n",
        "f_post = f_prior[:,post[:,0]]\n",
        "mean_post = f_post.mean(axis=1)\n",
        "var_post =  f_post.var(axis=1)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(Xshow, mean_prior, c='black')\n",
        "plt.gca().fill_between(Xshow.flat, mean_prior+var_prior, mean_prior-var_prior,\n",
        "                    color ='grey', alpha=0.2 )\n",
        "for i in range(m):\n",
        "  plt.plot(Xshow, f_prior[:,i], '-o', c='tab:blue',  alpha=0.3 );\n",
        "plt.ylim(-3,3);\n",
        "plt.xlim(0,1);\n",
        "plt.xlabel(\"x\");\n",
        "plt.title(\"Samples of functions from prior\");\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(Xshow, mean_post, c='black')\n",
        "plt.gca().fill_between(Xshow.flat, mean_post+var_post, mean_post-var_post,\n",
        "                    color ='grey', alpha=0.2 )\n",
        "for i in range(min(m, post.shape[0])):\n",
        "  plt.plot(Xshow, f_post[:,i], '-o', c='tab:orange',  alpha=0.3 );\n",
        "plt.plot(Xshow[int(n/2)], true_val, 'k+', markersize=25, markeredgewidth=3);\n",
        "\n",
        "plt.ylim(-3,3);\n",
        "plt.xlim(0,1);\n",
        "plt.xlabel(\"x\");\n",
        "plt.title(\"Samples of functions from posterior, after observing t(0.5) = 1\");\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L-IG7Iqvctr"
      },
      "source": [
        "Furthermore, given this observation, the mean (*across the posterior*) has changed from the constant mean across the prior - now our model will predict that the value at $x = 0.5$ should be 1. (Note that because of the kernel function, the \"mean\" line - which is the prediction of our Gaussian process regression model - is smooth.)\n",
        "\n",
        "Meanwhile, in the parts of the space that we have not sampled yet, the kernel function tells us what values of the function are likely, given the points we *have* sampled and the covariance function.\n",
        "\n",
        "\n",
        "As we sample more points, our \"mean\" function will start to take on a shape that fits the data well, thus (hopefully) modeling the true function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJMW1y1awnGQ"
      },
      "source": [
        "And, the standard deviation across the functions *in our posterior* is no longer constant for all values of $x$:\n",
        "\n",
        "* Near our observation at $x = 0.5$, we have very little uncertainty in our estimate of the true function - the functions in the posterior agree on the values there.\n",
        "* However, there is still variance among those functions as we get farther away from the observatoin at $x = 0.5$, so the uncertainty of the model is still large there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8r1zbLd42ad"
      },
      "source": [
        "Now we can understand both the prediction of the Gaussian Process Regression, and its built-in measure of uncertainty -\n",
        "\n",
        "* the mean of the posterior (i.e. after sampling some training points) is the model prediction\n",
        "* the variance across all the \"likely\" functions in the posterior tells us how confident we are in our prediction. In the above image, the blue shaded regions show the variance, or uncertainty, of our prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWWGVYIaAWtY"
      },
      "source": [
        "#### RBF Kernel\n",
        "\n",
        "In this experiment, we will use an RBF kernel in our Gaussian Process Regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvkcD412Bdw7"
      },
      "source": [
        "We had previously introduced an RBF kernel in the context of an SVM classifier, with\n",
        "\n",
        "$$k(x_i,x_j) = \\text{exp}(-\\gamma || x_i-x_j ||^2)$$\n",
        "\n",
        "and we said that if $\\gamma = \\frac{1}{\\sigma^{2}}$, this is known as the Gaussian kernel with variance $\\sigma^2$.\n",
        "\n",
        "We will use the same RBF kernel, but the Gaussian Process Regression implementation uses a `length_scale` hyperparameter $l$ for the kernel in place of the $\\gamma$ hyperparameter.  For the Gaussian Process Regression, the RBF kernel is\n",
        "\n",
        "$$k(x_i,x_j)  = \\exp\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)$$\n",
        "\n",
        "where $d(.)$ is the Euclidean distance and $l$ is the length scale hyperparameter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX-zBxjrBfIB"
      },
      "source": [
        "We can see that there is an inversely proportional relationship between $\\gamma$ and $l$. Therefore, with reference to our previous Colab lesson on SVM,\n",
        "\n",
        "* in the context of the SVM, we saw that when $\\gamma$ is large the decision surface is more complex. Now, because of that inverse relationship, we will note that when $l$ is *small* the function surface is more complex.\n",
        "* and we saw that when $\\gamma$ is small the decision surface is smoothed and less complex. Now, because of that inverse relationship, we will note that when $l$ is *large* the function surface is smoothed and less complex.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BlxJVUM6TF5"
      },
      "source": [
        "One important point to note is that in our Gaussian Process Regression, we will *fit* the kernel hyperparameters - we will try several values and select the value of $l$ that best explains the data, rather than specifying a fixed value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9OOGwaQ3vfO"
      },
      "source": [
        "#### Bayesian Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHYG1T7y233r"
      },
      "source": [
        "Bayesian Optimization is a technique in which we use information we have already gathered about a function, in order to decide which points to sample next so as to maximize the function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCiZLQHb4u_i"
      },
      "source": [
        "We had previously introduced Bayesian Optimization approach in the context of hyperparameter optimization, where instead of sampling the hyperparameter space uniformly in a grid, we iteratively:\n",
        "\n",
        " * sample a point in the hyperparameter space (here, \"sample a point\" means \"train a model with those hyperparameters, and get the validation score of the model\")\n",
        " * decide what point to sample next using a *utility* function, in order to balance\n",
        "   * exploration of the hyperparameter space, to avoid missing out on an optimal value that isn't close to areas we have sampled already\n",
        "   * versus focus on the areas where we believe (based on what we have seen so far) that the best model is likely to be.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmZxeA-z5HzX"
      },
      "source": [
        "But, this approach is well suited to a wide variety of problems in addition to hyperparemeter optimization - including our wireless localization problem! Some characteristics of this problem that make it well suited for Bayesian Optimization include:\n",
        "\n",
        "* we want to find the maximum of a function $y = f(x)$, but we don't necessarily care much about correctly estimating the value of $f(x)$ anywhere else (here $y$ is received signal strength, as a function of latitude and longitude). This technique lets us focus on the areas where $f(x)$ is largest.\n",
        "* it is \"expensive\" to sample a point from the function, i.e. to get $y$ for some $x$ (here, we have to fly a UAV to that coordinate - using precious, limited flight time! - in order to sample the received signal strength at that coordinate). This technique is designed to find the maximum of $f(x)$ in a very small number of samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP9DRADM7UBb"
      },
      "source": [
        "#### Putting it together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFTJugmI7XFz"
      },
      "source": [
        "Underneath the hood, our Bayesian Optimization is going to work by fitting a Gaussian Process Regression model (with an RBF kernel)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hEMffe9Ry9W"
      },
      "source": [
        "Here's how the experiment will work. First, the UAV will take off, and fly to the middle of the UGV search area. It will take a measurement of received signal power.\n",
        "\n",
        "Then, it will repeat in a loop:\n",
        "\n",
        "* report the most recent measurement (and the location of this measurement) to update the Gaussian Process Regression model.\n",
        "* the Bayes search optimizer will then suggest which location to fly to next\n",
        "* the UAV will fly to this location and measure received signal power\n",
        "\n",
        "until 5 minutes of flight time have elapsed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npj2-M858XNF"
      },
      "source": [
        "## Example with synthetic data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyfh9cyw8iPh"
      },
      "source": [
        "Let's see how it all works with an example on synthetic data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyfl4Fmp021r"
      },
      "source": [
        "### Ground truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPJuwhr48qq4"
      },
      "source": [
        "We are going to search for our hikers within the rectangle defined by the following bounds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGciJElH8u9s"
      },
      "outputs": [],
      "source": [
        "BOUND_NE={'lat':35.73030799378120, 'lon':-78.69670002283071}\n",
        "BOUND_NW={'lat':35.73030799378120, 'lon':-78.69980159100491}\n",
        "BOUND_SE={'lat':35.72774492720433, 'lon':-78.69670002283071}\n",
        "BOUND_SW={'lat':35.72774492720433, 'lon':-78.69980159100491}\n",
        "\n",
        "MAX_LON = BOUND_NE['lon']\n",
        "MIN_LON = BOUND_NW['lon']\n",
        "MAX_LAT = BOUND_NE['lat']\n",
        "MIN_LAT = BOUND_SE['lat']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swL3SZuI8wm3"
      },
      "source": [
        "Suppose that the \"true\" position of the hikers is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gru29dc8y_n"
      },
      "outputs": [],
      "source": [
        "np.random.seed(150)\n",
        "true_lat = np.random.uniform(BOUND_SE['lat'], BOUND_NE['lat'])\n",
        "true_lon = np.random.uniform(BOUND_SE['lon'], BOUND_SW['lon'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gQGoakH9RbQ"
      },
      "outputs": [],
      "source": [
        "plt.plot(true_lon, true_lat, 'r*', markersize=15);\n",
        "plt.xlabel('Longitude');\n",
        "plt.ylabel('Latitude');\n",
        "plt.xlim((BOUND_NW['lon'], BOUND_NE['lon']))  ;\n",
        "plt.ylim((BOUND_SE['lat'], BOUND_NE['lat']))  ;\n",
        "plt.ticklabel_format(useOffset=False) ;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UugjVKM1-DBw"
      },
      "source": [
        "Let's say that the observed data (the received signal strength) anywhere in this region is generated by the following function. Of course, we don't *know* this function - we can only observe samples from this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGbw39-A-QAA"
      },
      "outputs": [],
      "source": [
        "noise = 1\n",
        "\n",
        "def black_box_function(lat, lon):\n",
        "  distance = 1e3*np.sqrt((lat - true_lat)**2 + (lon - true_lon)**2)\n",
        "  received_signal_strength_dbm  =  - 20 * np.log10(distance + 0.00001) + 35\n",
        "  return np.clip(received_signal_strength_dbm, -20, 45) + np.random.normal(0, noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy76STxn_ZQO"
      },
      "source": [
        "If we could fly our UAV around the search space for a few days, constantly sampling received signal strenght, we'd see something like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6d_dBZq_Gql"
      },
      "outputs": [],
      "source": [
        "lat_grid = np.linspace(BOUND_SE['lat'], BOUND_NE['lat'], 100)\n",
        "lon_grid = np.linspace(BOUND_SE['lon'], BOUND_SW['lon'], 100)\n",
        "\n",
        "samples = np.zeros((len(lat_grid), len(lon_grid)))\n",
        "for i, lat in enumerate(lat_grid):\n",
        "    for j, lon in enumerate(lon_grid):\n",
        "        samples[i, j] = black_box_function(lat, lon)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.contourf(lon_grid, lat_grid, samples, cmap='viridis', vmin=20, vmax=50, levels=8)\n",
        "plt.colorbar(label='Received Signal Strength')\n",
        "\n",
        "plt.plot(true_lon, true_lat, 'r*', markersize=15);\n",
        "plt.xlabel('Longitude');\n",
        "plt.ylabel('Latitude');\n",
        "plt.xlim((BOUND_NW['lon'], BOUND_NE['lon']))  ;\n",
        "plt.ylim((BOUND_SE['lat'], BOUND_NE['lat']))  ;\n",
        "plt.ticklabel_format(useOffset=False) ;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ7LNG8MBQo9"
      },
      "source": [
        "But, let's see if we can find our hikers (the red star) without flying around for a few days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG45MQqx06aD"
      },
      "source": [
        "### Set up model and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd1s6ayQBfQP"
      },
      "source": [
        "First, we will define:\n",
        "\n",
        "* the *utility* function or *acquisition* function that the optimization will use, to decide which point to sample next. We will use an \"upper confidence bound\" function, which is a weighted sum of the mean of the Gaussian Process Regression ($\\mu(x)$) and its uncertainty ($\\sigma(x)$); the $\\kappa$ value determines the tradeoff between sampling values of $x$ where we expect $x$ to be large, and sampling values of $x$ about which we are uncertain.\n",
        "\n",
        "$$ \\mu(x) + \\kappa \\sigma (x)$$\n",
        "\n",
        "* the *kernel* that we will use for the Gaussian Process Regression. We will use the familiar RBF kernel, and we will specify that the length scale hyperparameter (which controls how wide or how narrow the curve is) should be in a particular range. Our optimizer will try a few different values in that range, to find the best one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2730wibCOd4"
      },
      "outputs": [],
      "source": [
        "utility = acquisition.UpperConfidenceBound()\n",
        "kernel = RBF(length_scale_bounds = (1e-04, 1e2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6pPeugWCjfG"
      },
      "source": [
        "Now we will define a Bayesian optimizer, and set up the kernel in the Gassian Process Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE7_HVJGCoif"
      },
      "outputs": [],
      "source": [
        "optimizer = BayesianOptimization(\n",
        "  f=None,\n",
        "  pbounds={'lat': (BOUND_SE['lat'], BOUND_NE['lat']), 'lon': (BOUND_SW['lon'], BOUND_SE['lon'])},\n",
        "  verbose=0,\n",
        "  random_state=50,\n",
        "  allow_duplicate_points=True,\n",
        "  acquisition_function = utility\n",
        ")\n",
        "optimizer._gp.set_params(kernel = kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqcFUfJPQfEu"
      },
      "source": [
        "Here are the initial kernel hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1fMBbLvQgmD"
      },
      "outputs": [],
      "source": [
        "optimizer._gp.kernel.get_params()['length_scale']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTaI0ouLz-1k"
      },
      "source": [
        "Note that our `GaussianProcessRegressor` has `n_restarts_optimizer = 5`. This determines how many different values of the kernel hyperparameter will be considered each time we update the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpLIL8Bc9Aid"
      },
      "source": [
        "We will set up some plotting functions to help us visualize the model -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAnRhaywQUY_"
      },
      "outputs": [],
      "source": [
        "def vis_optimizer():\n",
        "  lat_grid = np.linspace(BOUND_SE['lat'], BOUND_NE['lat'], 100)\n",
        "  lon_grid = np.linspace(BOUND_SE['lon'], BOUND_SW['lon'], 100)\n",
        "\n",
        "  coord_grid  = np.meshgrid(lat_grid, lon_grid)\n",
        "  coord_array = np.column_stack([coord_grid[0].ravel(), coord_grid[1].ravel()])\n",
        "\n",
        "  predictions = optimizer._gp.predict(coord_array, return_std=True)\n",
        "  gpr_mean = predictions[0].reshape(100, 100)\n",
        "  gpr_var  = predictions[1].reshape(100, 100)\n",
        "\n",
        "  lat_array = coord_array[:,0].reshape(100, 100)\n",
        "  lon_array = coord_array[:,1].reshape(100, 100)\n",
        "\n",
        "  try:\n",
        "    y_max = optimizer._space.target.max()\n",
        "  except:\n",
        "    y_max = 0\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=2, sharex=True, sharey=True, figsize=(10, 8));\n",
        "\n",
        "  axes[0, 0].contourf(lon_array, lat_array, gpr_mean, cmap='viridis', vmin=20, vmax=50, levels=8);\n",
        "  axes[0, 0].set_title(\"Mean of predicted received signal strength\");\n",
        "\n",
        "  axes[0, 1].contourf(lon_grid, lat_grid, samples, cmap='viridis', vmin=20, vmax=50, levels=8);\n",
        "  axes[0, 1].set_title(\"Actual function\");\n",
        "  try:\n",
        "    for i, x_train in enumerate(optimizer._gp.X_train_):\n",
        "      axes[0, 1].text(x_train[1], x_train[0], str(i), color=\"white\", fontsize=8, fontweight='heavy', ha=\"center\", va=\"center\",\n",
        "             bbox = dict(boxstyle=f\"circle\", fc=\"black\"))\n",
        "  except AttributeError:\n",
        "    pass\n",
        "\n",
        "  axes[1, 0].contourf(lon_array, lat_array, gpr_var, cmap='viridis');\n",
        "  axes[1, 0].set_title(\"Variance of predicted received signal strength\");\n",
        "\n",
        "  try:\n",
        "    util  = utility._get_acq(gp=optimizer._gp)(coord_array).reshape(100, 100)\n",
        "    axes[1, 1].contourf(lon_array, lat_array, util, cmap='viridis_r', vmin=-50, vmax=-20, levels=8);\n",
        "  except AttributeError:\n",
        "    pass\n",
        "  try:\n",
        "      axes[1, 1].plot(next_point_to_probe['lon'], next_point_to_probe['lat'], marker='P', markersize=10, markerfacecolor='red', markeredgecolor='white');\n",
        "  except NameError:\n",
        "    pass\n",
        "  axes[1, 1].set_title(\"Value of utility function\");\n",
        "\n",
        "  try:\n",
        "    for i, x_train in enumerate(optimizer._gp.X_train_):\n",
        "      axes[0, 0].plot(x_train[1], x_train[0], marker='P', markersize=10, markerfacecolor='black', markeredgecolor='white');\n",
        "      axes[1, 0].plot(x_train[1], x_train[0], marker='P', markersize=10, markerfacecolor='black', markeredgecolor='white');\n",
        "      axes[1, 1].plot(x_train[1], x_train[0], marker='P', markersize=10, markerfacecolor='black', markeredgecolor='white');\n",
        "  except AttributeError:\n",
        "    pass\n",
        "\n",
        "  axes[0, 0].plot(true_lon, true_lat, marker='*', color='red', markersize=15)\n",
        "  axes[0, 1].plot(true_lon, true_lat, marker='*', color='red', markersize=15)\n",
        "  axes[1, 0].plot(true_lon, true_lat, marker='*', color='red', markersize=15)\n",
        "  axes[1, 1].plot(true_lon, true_lat, marker='*', color='red', markersize=15)\n",
        "\n",
        "  try:\n",
        "      n_samples = optimizer._gp.X_train_.shape[0]\n",
        "  except AttributeError:\n",
        "      n_samples = 0\n",
        "\n",
        "  true_pos = (true_lat, true_lon)\n",
        "  est_pos  = (coord_array[np.argmax(gpr_mean)][0], coord_array[np.argmax(gpr_mean)][1])\n",
        "  est_error = geopy.distance.geodesic(true_pos, est_pos).m\n",
        "  plt.suptitle(\"Optimizer state after %d samples, error: %f m\" % (n_samples, est_error));\n",
        "\n",
        "  axes[0, 0].plot(est_pos[1], est_pos[0], marker='X', markersize=15, markerfacecolor='black', markeredgecolor='white');\n",
        "\n",
        "  plt.ticklabel_format(useOffset=False) ;\n",
        "  plt.tight_layout();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jlOHI1Y8wLD"
      },
      "outputs": [],
      "source": [
        "def plot_3D(elev=20, azim=-70):\n",
        "\n",
        "  lat_grid = np.linspace(BOUND_SE['lat'], BOUND_NE['lat'], 100)\n",
        "  lon_grid = np.linspace(BOUND_SE['lon'], BOUND_SW['lon'], 100)\n",
        "\n",
        "  coord_grid  = np.meshgrid(lat_grid, lon_grid)\n",
        "  coord_array = np.column_stack([coord_grid[0].ravel(), coord_grid[1].ravel()])\n",
        "\n",
        "  predictions = optimizer._gp.predict(coord_array, return_std=True)\n",
        "  gpr_mean = predictions[0].reshape(100, 100)\n",
        "  gpr_var  = predictions[1].reshape(100, 100)\n",
        "\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(projection = '3d')\n",
        "\n",
        "  lat_array = coord_array[:,0].reshape(100, 100)\n",
        "  lon_array = coord_array[:,1].reshape(100, 100)\n",
        "\n",
        "  ax.plot_wireframe(lon_array, lat_array, gpr_mean, color='black', alpha=0.3)\n",
        "  ax.plot_surface(lon_array, lat_array, gpr_mean, cmap=cm.viridis, alpha=0.7)\n",
        "\n",
        "  y = optimizer._gp.y_train_*optimizer._gp._y_train_std + optimizer._gp._y_train_mean\n",
        "  ax.scatter3D(optimizer._gp.X_train_[:,1], optimizer._gp.X_train_[:,0], y, c=y, edgecolor='black', s=50);\n",
        "\n",
        "  ax.ticklabel_format(useOffset=False)\n",
        "  ax.set_xlabel(\"Longitude\")\n",
        "  ax.set_ylabel(\"Latitude\")\n",
        "  ax.set_zlabel(\"Received Signal Strength\")\n",
        "\n",
        "  ax.view_init(elev=elev, azim=azim)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H03leFUdQKuf"
      },
      "source": [
        "then we can use that to see what our optimizer initially \"knows\" about the space that it is searching - nothing! But, as we begin to gather data, the model will start to improve its predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq7RwbDZ6fzK"
      },
      "source": [
        "In the following plot:\n",
        "\n",
        "* the title tells us the current error of our model: how far is its estimate of the hikers' position from the true position?\n",
        "* in each panel, the true position of the hikers is indicated with a red star\n",
        "* the \"Actual function\" panel shows us samples of received signal strength from the \"true\" function over the search space. Since we are using synthetic data generated by us, we can get a value for each position in the search space and plot it.\n",
        "* the \"Mean of predicted received signal strength\" panel shows us what the Gaussian Process Regression model predicts for the expected signal strength at each position in the search space. For now, since the model has not seen any training samples, its prediction is 0 everywhere.\n",
        "* the \"Variance of predicted received signal strength\" panel shows us the variance in the model's estimate of the received signal strength at each position in the search space. For now, since the model has not seen any training samples, the variance is constant everywhere.\n",
        "* the \"Value of utility function\" panel shows us where the model thinks it should sample the search space, in order to find the maximum value. For now, since the model has not seen any training samples, we do not have any information about where to search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAi6H4MCQqoj"
      },
      "outputs": [],
      "source": [
        "vis_optimizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emsHSfH453aB"
      },
      "source": [
        "### First search iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ZST_GMESAG"
      },
      "source": [
        "We are ready to start our search for the hikers! First, we will ask the optimizer to suggest a point to sample (using the utility function):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifnv7R3uEV9L"
      },
      "outputs": [],
      "source": [
        "next_point_to_probe = optimizer.suggest()\n",
        "next_point_to_probe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wki47CMa7hJN"
      },
      "source": [
        "The point suggested by the utility function will be indicated with a red `+` sign:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyOLwBqoG56N"
      },
      "outputs": [],
      "source": [
        "# plot true value (red star) and next point to probe (blue +)\n",
        "plt.plot(true_lon, true_lat, 'r*', markersize=15);\n",
        "plt.plot(next_point_to_probe['lon'], next_point_to_probe['lat'], 'rP', markersize=15);\n",
        "plt.xlim((BOUND_NW['lon'], BOUND_NE['lon']))  ;\n",
        "plt.ylim((BOUND_SE['lat'], BOUND_NE['lat']))  ;\n",
        "\n",
        "plt.xlabel('Longitude');\n",
        "plt.ylabel('Latitude');\n",
        "plt.ticklabel_format(useOffset=False) ;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItgG_MBREzLW"
      },
      "source": [
        "Suppose we fly the UAV to that point, and measure $y$, the received signal strength:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPtr1YqAE229"
      },
      "outputs": [],
      "source": [
        "y = black_box_function(next_point_to_probe['lat'], next_point_to_probe['lon'])\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAbsIQg9RnCi"
      },
      "source": [
        "We tell the optimizer what we observed at that point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzikfiJtRp89"
      },
      "outputs": [],
      "source": [
        "optimizer.register(\n",
        "    params=next_point_to_probe,\n",
        "    target=y\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6l1WyAVTVoU"
      },
      "source": [
        "and get the next point to probe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPG3wR3MTWvE"
      },
      "outputs": [],
      "source": [
        "next_point_to_probe = optimizer.suggest()\n",
        "next_point_to_probe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSaY817FE_Mx"
      },
      "source": [
        "Let's now visualize the model state again - in the following plot:\n",
        "\n",
        "* we can see in the title whether or not the error of our model estimate has improved.\n",
        "* in the \"Actual\" function panel, we are also showing each point that we sampled - each training sample! - and the order in which we sampled them, using black circles. In the other panels, we show the sampled points using black '+' signs.\n",
        "* the \"Mean of predicted received signal strength\" panel will still show a constant value everywhere, but that value has changed - now, we predict the value that we just observed for every point in the search space.\n",
        "* the \"Variance of predicted received signal strength\" panel has changed! Our model is now more confident in its estimate of the received signal strength near the first sample. The variance of the predicted received signal strength is smallest near the sample it took, and increases as it gets farther from that location.\n",
        "* the \"Value of utility function\" panel shows us where the model thinks it should sample the search space, in order to find the maximum value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBRZXKIwTJ7C"
      },
      "outputs": [],
      "source": [
        "vis_optimizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JtjygGqntMF"
      },
      "source": [
        "We can also represent this in 3D, with the height showing the received signal strength.\n",
        "\n",
        "* the training samples are indicated as points\n",
        "* the prediction of the model is shown as a wireframe/surface\n",
        "\n",
        "This makes it clear that the model predicts the received signal strength from the first sample everywhere in the search space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZjppRB06OH8"
      },
      "outputs": [],
      "source": [
        "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
        "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=-70));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81gQByJpYRU6"
      },
      "source": [
        "The GPR kernel parameters can change as the model is fitted. Let's check on them for now -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLkKbwcgYT41"
      },
      "outputs": [],
      "source": [
        "optimizer._gp.kernel_.get_params()['length_scale']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ch-bc_p62Ls"
      },
      "source": [
        "### More search iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3EHbS9U2Tce"
      },
      "source": [
        "Let's do it again! We repeat the process of: visit the next point, tell the optimizer what we measured there, and get the next point to visit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sx1JnvU2UW3"
      },
      "outputs": [],
      "source": [
        "y = black_box_function(next_point_to_probe['lat'], next_point_to_probe['lon'])\n",
        "optimizer.register(\n",
        "    params=next_point_to_probe,\n",
        "    target=y\n",
        ")\n",
        "next_point_to_probe = optimizer.suggest()\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnoijEn3obI2"
      },
      "source": [
        "Now we know about the received signal strength at two locations that we have sampled - the variance is lowest there. We are also starting to see that the mean prediction and the value of the utility function is no longer constant across the search space, since we had two different measurements at different points.\n",
        "\n",
        "The black 'X' in the first panel shows our estimate of the hikers' location - based on the observations we have so far collected, where does our model predict that signal strength will be highest? (This is the maximum of the mean of the Gaussian Process Regression.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0N1hJUa-J6t"
      },
      "outputs": [],
      "source": [
        "vis_optimizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmsFSd49_Xyq"
      },
      "source": [
        "In our 3D visualization, we start to see the effect of the RBF kernel.\n",
        "\n",
        "Our current kernel hyperparameters (small $l$) say that\n",
        "\n",
        "* when very close to a training sample, the predicted signal strength should be similar to the value of the training sample,\n",
        "* but that the effect falls off very quickly.\n",
        "\n",
        "We will see how this changes as the optimizer gets new observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ9EBjCj6_oc"
      },
      "outputs": [],
      "source": [
        "optimizer._gp.kernel_.get_params()['length_scale']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrZH8vrw67Ju"
      },
      "outputs": [],
      "source": [
        "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
        "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=-70));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OSanPCiAOZv"
      },
      "source": [
        "Do it a third time -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r16COJL3ATiQ"
      },
      "outputs": [],
      "source": [
        "y = black_box_function(next_point_to_probe['lat'], next_point_to_probe['lon'])\n",
        "optimizer.register(\n",
        "    params=next_point_to_probe,\n",
        "    target=y\n",
        ")\n",
        "next_point_to_probe = optimizer.suggest()\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMw1EFgCCSsZ"
      },
      "source": [
        "Note that the black 'X' in the first panel is our model's current estimate of the position where the signal strength is highest - this is our estimate of the hikers' position.\n",
        "\n",
        "In just a few steps, we have found a pretty good estimate. We can see this reflected in the title of the plot, which shows the distance between our estimate and the true position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8kkNzEEAlDN"
      },
      "outputs": [],
      "source": [
        "vis_optimizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRuvFrdzDaaL"
      },
      "source": [
        "Depending on the data that is observed, the `length_scale` argument of the RBF kernel that is estimated to best fit the data may become larger, so the function learned by the model is smoother. We will be able to see this in the 3D plot, when it occurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kheTkLTwB_IE"
      },
      "outputs": [],
      "source": [
        "optimizer._gp.kernel_.get_params()['length_scale']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64aayimjAmaI"
      },
      "outputs": [],
      "source": [
        "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
        "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=-70));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODqFzXwyAe1e"
      },
      "source": [
        "Let's iterate a fourth time -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6ZkPiXWAV-3"
      },
      "outputs": [],
      "source": [
        "y = black_box_function(next_point_to_probe['lat'], next_point_to_probe['lon'])\n",
        "optimizer.register(\n",
        "    params=next_point_to_probe,\n",
        "    target=y\n",
        ")\n",
        "next_point_to_probe = optimizer.suggest()\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muWgoXOhAYLw"
      },
      "outputs": [],
      "source": [
        "vis_optimizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9MMHPf1ArO0"
      },
      "outputs": [],
      "source": [
        "optimizer._gp.kernel_.get_params()['length_scale']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZl2N_BkAggp"
      },
      "outputs": [],
      "source": [
        "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
        "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=-70));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OGpbHpmW7Xa"
      },
      "source": [
        "Let's repeat the process a few times, and visualize the result again -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FeodHp-dkzd"
      },
      "outputs": [],
      "source": [
        "for _ in range(5):\n",
        "    next_point_to_probe = optimizer.suggest()\n",
        "    y = black_box_function(next_point_to_probe['lat'], next_point_to_probe['lon'])\n",
        "    print(next_point_to_probe['lat'], next_point_to_probe['lon'], y)\n",
        "    optimizer.register(params=next_point_to_probe, target=y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_COIOA04XED2"
      },
      "outputs": [],
      "source": [
        "vis_optimizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzECRQ-TDt6z"
      },
      "source": [
        "You can see that the optimizer sometimes visits parts of the search space where it has seen high signal strength, and sometimes visits unexplored parts of the search space (to make sure it is not missing a larger maximum in another area.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InA3c6I7CCBe"
      },
      "outputs": [],
      "source": [
        "optimizer._gp.kernel_.get_params()['length_scale']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL9er3BH7oNJ"
      },
      "outputs": [],
      "source": [
        "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
        "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=-70));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhIAlZ6_p14x"
      },
      "source": [
        "We'll do it a few more times -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_Eu25-Ap256"
      },
      "outputs": [],
      "source": [
        "for _ in range(5):\n",
        "    next_point_to_probe = optimizer.suggest()\n",
        "    y = black_box_function(next_point_to_probe['lat'], next_point_to_probe['lon'])\n",
        "    print(next_point_to_probe['lat'], next_point_to_probe['lon'], y)\n",
        "    optimizer.register(params=next_point_to_probe, target=y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbxD0BAyp6WJ"
      },
      "outputs": [],
      "source": [
        "vis_optimizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3A2mWSnMqGbp"
      },
      "outputs": [],
      "source": [
        "optimizer._gp.kernel_.get_params()['length_scale']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2OZGrnSp_Ws"
      },
      "outputs": [],
      "source": [
        "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
        "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=-70));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV1N_SFcqS6Q"
      },
      "source": [
        "After a while, our estimate of the hikers' position (and the error of the model) will start to stabliize. But because of the noise in the training samples, our prediction still has some error.\n",
        "\n",
        "Furthermore, our Gaussian Process Regression may start to prefer a smaller `length_scale` hyperparameter and overfit to the noise in the training samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n53GF5P6EMHR"
      },
      "outputs": [],
      "source": [
        "for _ in range(50):\n",
        "    next_point_to_probe = optimizer.suggest()\n",
        "    y = black_box_function(next_point_to_probe['lat'], next_point_to_probe['lon'])\n",
        "    print(next_point_to_probe['lat'], next_point_to_probe['lon'], y)\n",
        "    optimizer.register(params=next_point_to_probe, target=y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w1BpyyQEOs8"
      },
      "outputs": [],
      "source": [
        "vis_optimizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7aJ7XL0ERgN"
      },
      "outputs": [],
      "source": [
        "optimizer._gp.kernel_.get_params()['length_scale']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfvztDNiES6v"
      },
      "outputs": [],
      "source": [
        "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
        "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=-70));\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
